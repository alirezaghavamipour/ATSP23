{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The server is running now!\n",
      "Base model sent to the new client!\n",
      "Base model sent to the new client!\n",
      "Base model sent to the new client!\n",
      "Base model sent to the new client!\n",
      "Base model sent to the new client!\n",
      "Base model sent to the new client!\n",
      "Base model sent to the new client!\n",
      "Base model sent to the new client!\n",
      "Base model sent to the new client!\n",
      "Base model sent to the new client!\n",
      "Training round started\n",
      "1\n",
      "1\n",
      "Training round started\n",
      "2\n",
      "2\n",
      "Training round started\n",
      "3\n",
      "3\n",
      "Training round started\n",
      "4\n",
      "4\n",
      "Training round started\n",
      "5\n",
      "5\n",
      "Training round started\n",
      "6\n",
      "6\n",
      "Training round started\n",
      "7\n",
      "7\n",
      "Training round started\n",
      "8\n",
      "8\n",
      "Training round started\n",
      "9\n",
      "9\n",
      "Training round started\n",
      "10\n",
      "10\n",
      "Enough data recevied\n",
      "Avgg generator encrypted computed\n",
      "Averaging overhead: 00:00:00.00\n",
      "Plain data size in bytes 429.99 KB\n",
      "Avaraged dicriminator 1.19 MB\n",
      "Sent!\n",
      "Training round started\n",
      "1\n",
      "1\n",
      "Training round started\n",
      "2\n",
      "2\n",
      "Training round started\n",
      "3\n",
      "3\n",
      "Training round started\n",
      "4\n",
      "4\n",
      "Training round started\n",
      "5\n",
      "5\n",
      "Training round started\n",
      "6\n",
      "6\n",
      "Training round started\n",
      "7\n",
      "7\n",
      "Training round started\n",
      "8\n",
      "8\n",
      "Training round started\n",
      "9\n",
      "9\n",
      "Training round started\n",
      "10\n",
      "10\n",
      "Enough data recevied\n",
      "Avgg generator encrypted computed\n",
      "Averaging overhead: 00:00:00.00\n",
      "Plain data size in bytes 429.98 KB\n",
      "Avaraged dicriminator 1.19 MB\n",
      "Sent!\n",
      "Training round started\n",
      "1\n",
      "1\n",
      "Training round started\n",
      "2\n",
      "2\n",
      "Training round started\n",
      "3\n",
      "3\n",
      "Training round started\n",
      "4\n",
      "4\n",
      "Training round started\n",
      "5\n",
      "5\n",
      "Training round started\n",
      "6\n",
      "6\n",
      "Training round started\n",
      "7\n",
      "7\n",
      "Training round started\n",
      "8\n",
      "8\n",
      "Training round started\n",
      "9\n",
      "9\n",
      "Training round started\n",
      "10\n",
      "10\n",
      "Enough data recevied\n",
      "Avgg generator encrypted computed\n",
      "Averaging overhead: 00:00:00.00\n",
      "Plain data size in bytes 429.98 KB\n",
      "Avaraged dicriminator 1.19 MB\n",
      "Sent!\n",
      "Training round started\n",
      "1\n",
      "1\n",
      "Training round started\n",
      "2\n",
      "2\n",
      "Training round started\n",
      "3\n",
      "3\n",
      "Training round started\n",
      "4\n",
      "4\n",
      "Training round started\n",
      "5\n",
      "5\n",
      "Training round started\n",
      "6\n",
      "6\n",
      "Training round started\n",
      "7\n",
      "7\n",
      "Training round started\n",
      "8\n",
      "8\n",
      "Training round started\n",
      "9\n",
      "9\n",
      "Training round started\n",
      "10\n",
      "10\n",
      "Enough data recevied\n",
      "Avgg generator encrypted computed\n",
      "Averaging overhead: 00:00:00.00\n",
      "Plain data size in bytes 429.98 KB\n",
      "Avaraged dicriminator 1.19 MB\n",
      "Sent!\n",
      "Training round started\n",
      "1\n",
      "1\n",
      "Training round started\n",
      "2\n",
      "2\n",
      "Training round started\n",
      "3\n",
      "3\n",
      "Training round started\n",
      "4\n",
      "4\n",
      "Training round started\n",
      "5\n",
      "5\n",
      "Training round started\n",
      "6\n",
      "6\n",
      "Training round started\n",
      "7\n",
      "7\n",
      "Training round started\n",
      "8\n",
      "8\n",
      "Training round started\n",
      "9\n",
      "9\n",
      "Training round started\n",
      "10\n",
      "10\n",
      "Enough data recevied\n",
      "Avgg generator encrypted computed\n",
      "Averaging overhead: 00:00:00.00\n",
      "Plain data size in bytes 429.98 KB\n",
      "Avaraged dicriminator 1.19 MB\n",
      "Sent!\n",
      "Training round started\n",
      "1\n",
      "1\n",
      "Training round started\n",
      "2\n",
      "2\n",
      "Training round started\n",
      "3\n",
      "3\n",
      "Training round started\n",
      "4\n",
      "4\n",
      "Training round started\n",
      "5\n",
      "5\n",
      "Training round started\n",
      "6\n",
      "6\n",
      "Training round started\n",
      "7\n",
      "7\n",
      "Training round started\n",
      "8\n",
      "8\n",
      "Training round started\n",
      "9\n",
      "9\n",
      "Training round started\n",
      "10\n",
      "10\n",
      "Enough data recevied\n",
      "Avgg generator encrypted computed\n",
      "Averaging overhead: 00:00:00.00\n",
      "Plain data size in bytes 429.98 KB\n",
      "Avaraged dicriminator 1.19 MB\n",
      "Sent!\n",
      "Training round started\n",
      "1\n",
      "1\n",
      "Training round started\n",
      "2\n",
      "2\n",
      "Training round started\n",
      "3\n",
      "3\n",
      "Training round started\n",
      "4\n",
      "4\n",
      "Training round started\n",
      "5\n",
      "5\n",
      "Training round started\n",
      "6\n",
      "6\n",
      "Training round started\n",
      "7\n",
      "7\n",
      "Training round started\n",
      "8\n",
      "8\n",
      "Training round started\n",
      "9\n",
      "9\n",
      "Training round started\n",
      "10\n",
      "10\n",
      "Enough data recevied\n",
      "Avgg generator encrypted computed\n",
      "Averaging overhead: 00:00:00.03\n",
      "Plain data size in bytes 429.98 KB\n",
      "Avaraged dicriminator 1.19 MB\n",
      "Sent!\n",
      "Training round started\n",
      "1\n",
      "1\n",
      "Training round started\n",
      "2\n",
      "2\n",
      "Training round started\n",
      "3\n",
      "3\n",
      "Training round started\n",
      "4\n",
      "4\n",
      "Training round started\n",
      "5\n",
      "5\n",
      "Training round started\n",
      "6\n",
      "6\n",
      "Training round started\n",
      "7\n",
      "7\n",
      "Training round started\n",
      "8\n",
      "8\n",
      "Training round started\n",
      "9\n",
      "9\n",
      "Training round started\n",
      "10\n",
      "10\n",
      "Enough data recevied\n",
      "Avgg generator encrypted computed\n",
      "Averaging overhead: 00:00:00.00\n",
      "Plain data size in bytes 429.97 KB\n",
      "Avaraged dicriminator 1.19 MB\n",
      "Sent!\n",
      "Training round started\n",
      "1\n",
      "1\n",
      "Training round started\n",
      "2\n",
      "2\n",
      "Training round started\n",
      "3\n",
      "3\n",
      "Training round started\n",
      "4\n",
      "4\n",
      "Training round started\n",
      "5\n",
      "5\n",
      "Training round started\n",
      "6\n",
      "6\n",
      "Training round started\n",
      "7\n",
      "7\n",
      "Training round started\n",
      "8\n",
      "8\n",
      "Training round started\n",
      "9\n",
      "9\n",
      "Training round started\n",
      "10\n",
      "10\n",
      "Enough data recevied\n",
      "Avgg generator encrypted computed\n",
      "Averaging overhead: 00:00:00.00\n",
      "Plain data size in bytes 429.97 KB\n",
      "Avaraged dicriminator 1.19 MB\n",
      "Sent!\n",
      "Training round started\n",
      "1\n",
      "1\n",
      "Training round started\n",
      "2\n",
      "2\n",
      "Training round started\n",
      "3\n",
      "3\n",
      "Training round started\n",
      "4\n",
      "4\n",
      "Training round started\n",
      "5\n",
      "5\n",
      "Training round started\n",
      "6\n",
      "6\n",
      "Training round started\n",
      "7\n",
      "7\n",
      "Training round started\n",
      "8\n",
      "8\n",
      "Training round started\n",
      "9\n",
      "9\n",
      "Training round started\n",
      "10\n",
      "10\n",
      "Enough data recevied\n",
      "Avgg generator encrypted computed\n",
      "Averaging overhead: 00:00:00.00\n",
      "Plain data size in bytes 429.99 KB\n",
      "Avaraged dicriminator 1.19 MB\n",
      "Sent!\n",
      "Total Traning Time: 00:17:41.78\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy\n",
    "from collections import OrderedDict\n",
    "import zmq\n",
    "import sys\n",
    "import threading\n",
    "import time\n",
    "from random import randint, random\n",
    "import time\n",
    "import zmq\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "numpy.set_printoptions(suppress=False)\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "import tenseal as ts\n",
    "import pickle\n",
    "\n",
    "\n",
    "################################################################\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from torchsummary import summary\n",
    "import numpy as np\n",
    "import torchvision.utils as vutils\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "import math\n",
    "\n",
    "def convert_size(size_bytes):\n",
    "    if size_bytes == 0:\n",
    "        return \"0B\"\n",
    "    size_name = (\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\", \"EB\", \"ZB\", \"YB\")\n",
    "    i = int(math.floor(math.log(size_bytes, 1024)))\n",
    "    p = math.pow(1024, i)\n",
    "    s = round(size_bytes / p, 2)\n",
    "    return \"%s %s\" % (s, size_name[i])\n",
    "\n",
    "\n",
    "ngpu = 1 \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, no_of_channels=1, disc_dim=32):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "\n",
    "                nn.Conv2d(in_channels=no_of_channels, out_channels=disc_dim, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "                nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
    "\n",
    "                nn.Conv2d(in_channels=disc_dim, out_channels=disc_dim * 2, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "                nn.BatchNorm2d(disc_dim * 2, track_running_stats=False),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "                nn.Conv2d(in_channels=disc_dim * 2, out_channels=disc_dim * 4, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "                nn.BatchNorm2d(disc_dim * 4, track_running_stats=False),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "                nn.Conv2d(in_channels=disc_dim * 4, out_channels=1, \n",
    "                          kernel_size=4, stride=1, padding=0, bias=False),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "    def forward(self, input):\n",
    "        '''\n",
    "        forward pass of the discriminator\n",
    "        Input is an image tensor, \n",
    "        returns a 1-dimension tensor representing image as    \n",
    "        fake/real.\n",
    "        '''\n",
    "        output = self.network(input)\n",
    "        return output.view(-1, 1).squeeze(1)\n",
    "    \n",
    "    \n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.layers = nn.Sequential(*[\n",
    "                                      self.conv_block(100, 128, padding=0),\n",
    "                                      self.conv_block(128, 64, stride=2, ks=3),\n",
    "                                      self.conv_block(64, 32, stride=2),\n",
    "                                      self.conv_block(32, 1, stride=2, bn=False, out_layer=True)\n",
    "        ])\n",
    "        # Our input is 100 dimensional random noise\n",
    "\n",
    "    @staticmethod\n",
    "    def conv_block(in_c, out_c, out_layer=False, ks=4, stride=1, padding=1, bias=False, bn=True):\n",
    "        l = [nn.ConvTranspose2d(in_c, out_c, ks, stride=stride, padding=padding, bias=bias)]\n",
    "        if bn: l.append(nn.BatchNorm2d(out_c, track_running_stats=False))\n",
    "        if out_layer: l.append(nn.Tanh())\n",
    "        else: l.append(nn.ReLU(True))\n",
    "        return nn.Sequential(*l)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "G = Generator()\n",
    "D = Discriminator()\n",
    "\n",
    "# print(G)\n",
    "\n",
    "################################################################\n",
    "\n",
    "D.load_state_dict(torch.load('Dis_HE.ckpt'))\n",
    "discrimiator = D.state_dict()\n",
    "\n",
    "\n",
    "# for k, v in discrimiator.items():\n",
    "#     print(k)\n",
    "    \n",
    "    \n",
    "G.load_state_dict(torch.load('Gen_HE.ckpt'))\n",
    "generator = G.state_dict()\n",
    "\n",
    "# for k, v in generator.items():\n",
    "#     print(k)\n",
    "\n",
    "vals111 = []\n",
    "keys = []\n",
    "vals = []\n",
    "for k, v in discrimiator.items():\n",
    "\n",
    "    keys.append(k)\n",
    "    vals.append(v)\n",
    "    a = v.numpy()\n",
    "    vals111.append(a.shape)\n",
    "    \n",
    "# print(vals111)\n",
    "# vals111 = numpy.array(vals)   \n",
    "# keys = numpy.array(keys)\n",
    "# vals = numpy.array(vals)\n",
    "\n",
    "\n",
    "\n",
    "keys1 = []\n",
    "vals1 = []\n",
    "for k1, v1 in generator.items():\n",
    "\n",
    "    keys1.append(k1)\n",
    "    vals1.append(v1)\n",
    "\n",
    "# keys1 = numpy.array(keys1)\n",
    "# vals1 = numpy.array(vals1)\n",
    "\n",
    "\n",
    "# print(\"Model's state_dict:\")\n",
    "# for param_tensor in model2.state_dict():\n",
    "#     print(torch.numel(model2.state_dict()[param_tensor]))\n",
    "    \n",
    "    \n",
    "\n",
    "###################################################################\n",
    "def elapsed_time_total(start, end):\n",
    "    hours, rem = divmod(end-start, 3600)\n",
    "    minutes, seconds = divmod(rem, 60)\n",
    "    print(\"Total Traning Time: {:0>2}:{:0>2}:{:05.2f}\"\n",
    "                .format(int(hours),int(minutes),seconds))\n",
    "\n",
    "def elapsed_time_avg(start, end):\n",
    "    hours, rem = divmod(end-start, 3600)\n",
    "    minutes, seconds = divmod(rem, 60)\n",
    "    print(\"Averaging overhead: {:0>2}:{:0>2}:{:05.2f}\"\n",
    "                .format(int(hours),int(minutes),seconds))\n",
    "\n",
    "    \n",
    "###################################################################\n",
    "\n",
    "\n",
    "import tenseal as ts\n",
    "\n",
    "import base64\n",
    "\n",
    "# Assuming UTF-8 encoding, change to something else if you need to\n",
    "base64.b64encode(\"password\".encode(\"utf-8\"))\n",
    "\n",
    "\n",
    "def write_data(file_name, data):\n",
    "    if type(data) == bytes:\n",
    "        #bytes to base64\n",
    "        data = base64.b64encode(data)\n",
    "         \n",
    "    with open(file_name, 'wb') as f: \n",
    "        f.write(data)\n",
    " \n",
    "def read_data(file_name):\n",
    "    with open(file_name, \"rb\") as f:\n",
    "        data = f.read()\n",
    "    #base64 to bytes\n",
    "    return base64.b64decode(data)\n",
    "\n",
    "\n",
    "###################################################################\n",
    "\n",
    "\n",
    "global data_list\n",
    "global client_num\n",
    "\n",
    "client_num = 0\n",
    "\n",
    "context = zmq.Context()\n",
    "socket = context.socket(zmq.ROUTER)\n",
    "socket.bind(\"tcp://*:5555\")\n",
    "\n",
    "\n",
    "\n",
    "pub_socket = context.socket(zmq.PUB)\n",
    "pub_socket.bind(\"tcp://*:5557\")\n",
    "\n",
    "\n",
    "start_total = time.time()\n",
    "\n",
    "print(\"The server is running now!\")\n",
    "\n",
    "c = 0\n",
    "data_list = []\n",
    "loaded_enc = []\n",
    "loaded_enc_tmp = []\n",
    "cipher1 = []\n",
    "cipher2 = []\n",
    "sum_ = 0\n",
    "\n",
    "data_list_dicriminator = []\n",
    "data_list_generator = []\n",
    "sum_1 = 0\n",
    "sum_2 = 0\n",
    "\n",
    "client_num_ = 10\n",
    "\n",
    "\n",
    "while c < client_num_ * 10:\n",
    "    \n",
    "#     print(G)\n",
    "    ident1, msg1 = socket.recv_multipart()\n",
    "    ident2, msg2 = socket.recv_multipart()\n",
    "        \n",
    "    string = b\"New\"\n",
    "    \n",
    "    if string == msg1 and string == msg2: \n",
    "        \n",
    "        client_num = client_num + 1\n",
    "        \n",
    "        message1 = pickle.dumps(vals)\n",
    "        socket.send_multipart([ident1, message1])\n",
    "        \n",
    "        message2 = pickle.dumps(vals1)\n",
    "        socket.send_multipart([ident2, message2])\n",
    "        \n",
    "        print(\"Base model sent to the new client!\")\n",
    "    \n",
    "    else: \n",
    "        \n",
    "        print(\"Training round started\")\n",
    "        \n",
    "        message1 = pickle.loads(msg1)\n",
    "        message2 = pickle.loads(msg2)\n",
    "        \n",
    "        if len(message1) == 8:\n",
    "            \n",
    "            data_list_dicriminator.append(message1)\n",
    "        else:\n",
    "            data_list_generator.append(message1)\n",
    "        \n",
    "        if len(message2) == 8:\n",
    "            data_list_dicriminator.append(message2)\n",
    "        else:\n",
    "            data_list_generator.append(message2)\n",
    "            \n",
    "            \n",
    "        print(len(data_list_dicriminator))\n",
    "        print(len(data_list_generator))\n",
    "\n",
    "\n",
    "        if len(data_list_dicriminator) == client_num_ and len(data_list_generator) == client_num_:\n",
    "            \n",
    "            print(\"Enough data recevied\")\n",
    "            \n",
    "\n",
    "            start_avg = time.time()\n",
    "        \n",
    "            cipher1 = sum(data_list_dicriminator) / client_num_\n",
    "            cipher2 = sum(data_list_generator) / client_num_\n",
    " \n",
    "\n",
    "            print(\"Avgg generator encrypted computed\")\n",
    "            end_avg = time.time()\n",
    "            elapsed_time_avg(start_avg, end_avg) \n",
    "            \n",
    "            message1 = pickle.dumps(cipher1)\n",
    "            print(\"Plain data size in bytes {}\".format(convert_size(len(message1))))\n",
    "\n",
    "            message2 = pickle.dumps(cipher2)\n",
    "            print(\"Avaraged dicriminator {}\".format(convert_size(len(message2))))\n",
    "\n",
    "            \n",
    "            pub_socket.send(message1)\n",
    "            pub_socket.send(message2)\n",
    "            \n",
    "            print(\"Sent!\")\n",
    "            \n",
    "\n",
    "            cipher1 = []\n",
    "            cipher2 = []\n",
    "            sum_1 = 0\n",
    "            sum_2 = 0\n",
    "            sum_final1 = 0\n",
    "            sum_final2 = 0\n",
    "            data_list_dicriminator = []\n",
    "            data_list_generator = []\n",
    "            \n",
    "        c = c + 1\n",
    "        \n",
    "end_total = time.time()\n",
    "elapsed_time_total(start_total, end_total) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    }
   ],
   "source": [
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.9.5",
   "language": "python",
   "name": "3.9.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
